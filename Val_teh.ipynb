{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ASO-Y8_ziujE",
        "gjYVqSpl2sfa",
        "9uLd3m8si94u"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaraldsU/VTP_LPD/blob/main/Val_teh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text masking"
      ],
      "metadata": {
        "id": "ASO-Y8_ziujE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Rf0z_OGSrocH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "MJMg1XACw9qD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5f22dd-65d9-4c2d-853a-ab707525a33e",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (1.8.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza) (2.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install line_profiler"
      ],
      "metadata": {
        "id": "oovNpO99txAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5e466a-7227-442d-9450-4616a16bc45f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: line_profiler in /usr/local/lib/python3.10/dist-packages (4.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext line_profiler"
      ],
      "metadata": {
        "id": "SdNVNYOmt_Ox"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/HaraldsU/VTP_LPD/main/Data/stopwords.txt\" -O 'stopwords.txt'"
      ],
      "metadata": {
        "id": "ZeBTV_d-G_ki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1d20a3-ba1a-40e5-a6ea-fdf34046c2bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-18 23:18:00--  https://raw.githubusercontent.com/HaraldsU/VTP_LPD/main/Data/stopwords.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681 [text/plain]\n",
            "Saving to: ‘stopwords.txt’\n",
            "\n",
            "stopwords.txt       100%[===================>]     681  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-18 23:18:01 (44.5 MB/s) - ‘stopwords.txt’ saved [681/681]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UA7lDLpeCykc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import re\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Download the Stanford CoreNLP package with Stanza's installation command\n",
        "# This'll take several minutes, depending on the network speed\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "from stanza.server import CoreNLPClient\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJgF8nuiJZlq",
        "outputId": "e6fa61d1-df1f-4d64-c232-eb0ca6e00c2f",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:stanza:Directory ./corenlp already exists. Please install CoreNLP to a new directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "zBGKntzUr0_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_text(output):\n",
        "  # print_help(output, 'Op')\n",
        "  regexp = re.compile(r'[MSK\\d*]')\n",
        "  regexp2 = re.compile(r\"'[A-Za-z]\")\n",
        "  final_output = []\n",
        "\n",
        "  for word in range(len(output)):\n",
        "    prev = output[word - 1]\n",
        "    pprev = output[word - 2]\n",
        "    cur = output[word]\n",
        "\n",
        "    if not regexp.search(cur):\n",
        "      if word == 0:\n",
        "        cur = cur.capitalize()\n",
        "      elif word > 1 and prev in ['.', '!', '?']:\n",
        "        cur = cur.capitalize()\n",
        "\n",
        "    if (cur in [\".\", \",\", '!', '?', ':', ';'] and final_output) or regexp2.search(cur):\n",
        "      final_output[-1] += cur\n",
        "    elif cur == '-':\n",
        "      final_output[-1] += cur\n",
        "      final_output[-1] += output[word + 1]\n",
        "      word += 1\n",
        "    elif word == len(output) - 1 and cur not in ['.', '?', '!', '\\'']:\n",
        "      final_output[-1] += '.'\n",
        "    else:\n",
        "        final_output.append(cur)\n",
        "\n",
        "  return ' '.join(final_output)"
      ],
      "metadata": {
        "id": "oDA1lV3ywA6N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spans_to_ranges(spans):\n",
        "  msk_gr = []\n",
        "  for span in spans:\n",
        "    span1_start = span['span1']['start']\n",
        "    span1_end = span['span1']['end']\n",
        "    span2_start = span['span2']['start']\n",
        "    span2_end = span['span2']['end']\n",
        "    score = span['score']\n",
        "    in_grp = False\n",
        "    c = 0\n",
        "    for gr in msk_gr:\n",
        "      for rng in gr:\n",
        "        if span1_start<=rng[1] and span1_end >= rng[0]:\n",
        "          c+=1\n",
        "        if span2_start<=rng[1] and span2_end >= rng[0]:\n",
        "          c+=2\n",
        "        if  c == 3 : break\n",
        "      if    c == 0 :\n",
        "        continue\n",
        "      elif  c == 1 :\n",
        "        gr.append([span2_start,span2_end,score])\n",
        "        break\n",
        "      elif  c == 2 :\n",
        "        gr.append([span1_start,span1_end,score])\n",
        "        break\n",
        "      elif  c == 3 :\n",
        "        break\n",
        "    # print(span)\n",
        "    # print(c, [span1_start,span1_end], [span2_start,span2_end])\n",
        "    if c == 0: msk_gr.append([[span1_start,span1_end,score],[span2_start,span2_end,score]])\n",
        "\n",
        "  # print('\\n----------------------------------------------')\n",
        "  # TODO: Check if sceanrio where ranges intersect exists, implement it\n",
        "  combined = True\n",
        "  while combined:\n",
        "    combined=False\n",
        "    for i in range(len(msk_gr)):\n",
        "      for r_i in msk_gr[i]:\n",
        "        for j in range(i+1,len(msk_gr)):\n",
        "          for r_j in msk_gr[j]:\n",
        "            if r_i[0]<=r_j[1] and r_i[1] >= r_j[0]:\n",
        "              # print(r_i, r_j)\n",
        "              combined = True\n",
        "              for r_jj in msk_gr[j]:\n",
        "                duplicate = None\n",
        "                for r_ii in msk_gr[i]:\n",
        "                  if r_ii[0]<=r_jj[1] and r_ii[1] >= r_jj[0]:\n",
        "                    duplicate = r_ii\n",
        "                    break\n",
        "                if(duplicate):\n",
        "                  r_ii == r_ii if r_ii[2]>r_jj[2] else r_jj\n",
        "                else:\n",
        "                  msk_gr[i].append(r_jj)\n",
        "              msk_gr.pop(j)\n",
        "            if(combined):break\n",
        "          if(combined):break\n",
        "        if(combined):break\n",
        "      if(combined):break\n",
        "\n",
        "  # print('----------------------------------------------')\n",
        "  return msk_gr"
      ],
      "metadata": {
        "id": "l-3TIASat5hT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_spans(spans, cos_threshold, model):\n",
        "  embedings =  model.encode([s[\"text\"] for s in spans])\n",
        "  similar_spans = []\n",
        "  for i in range(len(spans)-1):\n",
        "    emb = embedings[i]\n",
        "    for j in range(i+1,len(spans)):\n",
        "      if spans[j]['start'] <= spans[i]['end']:\n",
        "        continue\n",
        "      emb_chk = embedings[j]\n",
        "      cos_score = cosine_similarity([emb], [emb_chk])\n",
        "      if(cos_score>=cos_threshold):\n",
        "        score = 1*cos_score + 0.15*(1-1/(spans[i]['end']+1-spans[i]['start']+spans[j]['end']+1-spans[j]['start'])) # cos_score +  length of text\n",
        "\n",
        "        similar_spans.append({\n",
        "            'score': score, 'cos:':cos_score\n",
        "            ,'span1':spans[i]\n",
        "            ,'span2':spans[j]})\n",
        "  similar_spans = sorted(similar_spans, key=lambda d: d['score'], reverse=True)\n",
        "  return similar_spans"
      ],
      "metadata": {
        "id": "5nXmZXiitn9W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spans(words, stopwords):\n",
        "  spans=[]\n",
        "  regexp = re.compile(r'[MSK\\d*]')\n",
        "  range(len(words))\n",
        "  for i in range(len(words)):\n",
        "    for j in range(i+1, len(words)+1):\n",
        "      c = False\n",
        "      for w in words[i:j]:\n",
        "        if w in stopwords:\n",
        "          c = True\n",
        "          break\n",
        "      if c: continue\n",
        "\n",
        "      s = ' '.join(words[i:j])\n",
        "      # s=s.replace(' ,','')\n",
        "      if(regexp.search(s) or s ==','):\n",
        "        continue\n",
        "      spans.append({\"text\":s, \"start\":i, \"end\": j-1})\n",
        "  return spans"
      ],
      "metadata": {
        "id": "wKSBSedKsYWJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coref_mask_and_tokenize(text, core_nlp_client):\n",
        "  msk_id=0\n",
        "  document = core_nlp_client.annotate(text)\n",
        "  corefs = document.corefChain\n",
        "  # print(corefs)\n",
        "  resolved = []\n",
        "  chain_mask = {}\n",
        "  for chain in corefs:\n",
        "    if len(chain.mention) == 1:\n",
        "      continue\n",
        "    msk_id+=1\n",
        "    chain_mask[chain.chainID]='[MSK'+str(msk_id)+']'\n",
        "\n",
        "  words = []\n",
        "  words_lemmatized = []\n",
        "  for sentence in document.sentence:\n",
        "    for token in sentence.token:\n",
        "        corefClustId = token.corefClusterID\n",
        "        chain = None\n",
        "        for c in corefs:\n",
        "          if c.chainID == corefClustId:\n",
        "            chain = c\n",
        "            break\n",
        "\n",
        "        if chain is None or len(chain.mention) == 1:\n",
        "          word = token.word.lower()\n",
        "          word_l = token.lemma\n",
        "        else:\n",
        "          word = chain_mask.get(chain.chainID)\n",
        "          word_l = word\n",
        "        words.append(word)\n",
        "        words_lemmatized.append(word_l)\n",
        "  return words, words_lemmatized, msk_id\n"
      ],
      "metadata": {
        "id": "OjcDbdb-up03"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stopwords():\n",
        "    # Open the file and read the stopwords\n",
        "    with open(\"stopwords.txt\", \"r\") as text_file:\n",
        "        stopwords = text_file.read().splitlines()\n",
        "\n",
        "    return stopwords"
      ],
      "metadata": {
        "id": "VYO2R4KZyZ7D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_quotes(text):\n",
        "    result = []\n",
        "    for word in text:\n",
        "        new_word = word.replace('\"', '').replace('”', '').replace('“', '')\n",
        "        result.append(new_word)\n",
        "    return result"
      ],
      "metadata": {
        "id": "w9QVJs3_bH8N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_help(obj, name='Obj'):\n",
        "  print(name, '=', end=\" \")\n",
        "  for item in obj:\n",
        "    print(item, end =\" \"),\n",
        "  print()"
      ],
      "metadata": {
        "id": "bW1cawz0YA3-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_word_ranges(words, mask_ranges, msk_index):\n",
        "  d = dict(enumerate(map(str, words)))\n",
        "  for msk in mask_ranges:\n",
        "    # print(msk)\n",
        "    msk_index+=1\n",
        "    msk_txt = '[MSK'+str(msk_index)+']'\n",
        "    for r in msk:\n",
        "      d[r[0]]= msk_txt\n",
        "      for i in range(r[0]+1,r[1]+1):\n",
        "        if(i in d):\n",
        "          d.pop(i)\n",
        "  arr = [v for k, v in d.items()]\n",
        "  return arr"
      ],
      "metadata": {
        "id": "1qgib-BoT21X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_text_file(text_arr, core_nlp_client, out_file_name, write_mode, model):\n",
        "  stopwords = get_stopwords()\n",
        "\n",
        "  text_arr = remove_quotes(text_arr)\n",
        "\n",
        "  result = []\n",
        "  write_queue=[]\n",
        "  total = len(text_arr)\n",
        "  i = 0\n",
        "\n",
        "  for text in text_arr:\n",
        "    i+=1\n",
        "    words, words_lemmatized, mask_count = coref_mask_and_tokenize(text, core_nlp_client)\n",
        "    # print(words)\n",
        "    spans = get_spans(words_lemmatized, stopwords)\n",
        "    print(f'\\r{round((i / total) * 100, 2)}% / 100% (', i , ')', end='', flush=True)\n",
        "\n",
        "    if not spans:\n",
        "      result=text\n",
        "      write_queue.append(result+'\\n')\n",
        "      print('\\nNo valid spans')\n",
        "      continue\n",
        "\n",
        "    similar_spans = get_similar_spans(spans, .91, model)\n",
        "    mask_ranges = spans_to_ranges(similar_spans)\n",
        "\n",
        "    maksed_words = mask_word_ranges(words, mask_ranges, mask_count)\n",
        "    # maksed_words = mask_word_ranges(words_lemmatized, mask_ranges, mask_count)\n",
        "\n",
        "    result = format_text(maksed_words)\n",
        "    write_queue.append(result + '\\n')\n",
        "\n",
        "  write_queue_df = pd.DataFrame(write_queue)\n",
        "  write_queue_df.to_csv(out_file_name, index=False, quoting=1)\n",
        "  print('\\nFinished!')\n"
      ],
      "metadata": {
        "id": "ulfB7oCSr_XN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_text(text_arr, core_nlp_client, model):\n",
        "  stopwords = get_stopwords()\n",
        "  text_arr = remove_quotes(text_arr)\n",
        "  result = []\n",
        "  total = len(text_arr)\n",
        "\n",
        "  for text in text_arr:\n",
        "    words, words_lemmatized, mask_count = coref_mask_and_tokenize(text, core_nlp_client)\n",
        "    spans = get_spans(words_lemmatized, stopwords)\n",
        "    if not spans:\n",
        "      result.append(text)\n",
        "      continue\n",
        "\n",
        "    similar_spans = get_similar_spans(spans, .90, model)\n",
        "    mask_ranges = spans_to_ranges(similar_spans)\n",
        "\n",
        "    maksed_words = mask_word_ranges(words, mask_ranges, mask_count)\n",
        "    # maksed_words = mask_word_ranges(words_lemmatized, mask_ranges, mask_count)\n",
        "\n",
        "    result.append(format_text(maksed_words))\n",
        "\n",
        "  return  result"
      ],
      "metadata": {
        "id": "vY8eE9ANk5Pv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLI"
      ],
      "metadata": {
        "id": "gjYVqSpl2sfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/HaraldsU/VTP_LPD/main/Data/mapping.json -O 'mapping.json'"
      ],
      "metadata": {
        "id": "4RGqin2-3-ZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c881f79b-db91-4732-f827-8cee08882e7c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-18 23:18:08--  https://raw.githubusercontent.com/HaraldsU/VTP_LPD/main/Data/mapping.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1521 (1.5K) [text/plain]\n",
            "Saving to: ‘mapping.json’\n",
            "\n",
            "mapping.json        100%[===================>]   1.49K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-18 23:18:08 (27.6 MB/s) - ‘mapping.json’ saved [1521/1521]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pandas scikit-learn"
      ],
      "metadata": {
        "id": "XYOG1SnhURmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a137d2d-ac98-4907-e090-067cac8e7b6c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Y3oEZ2Kr2aYw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(file_name):\n",
        "  with open(file_name, 'r') as file:\n",
        "      candidate_labels = json.load(file)\n",
        "\n",
        "  # first_items = []\n",
        "  second_items = []\n",
        "\n",
        "  for key, value in candidate_labels.items():\n",
        "      # first_items.append(key)\n",
        "      second_items.append(value)\n",
        "\n",
        "  # print(second_items)\n",
        "  return second_items"
      ],
      "metadata": {
        "id": "B_DWjF7c3tj4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_mask_to_understandable(arr):\n",
        "  with open('mapping.json', 'r') as file:\n",
        "      labels = json.load(file)\n",
        "\n",
        "  first_items = list(labels.keys())\n",
        "  second_items = list(labels.values())\n",
        "\n",
        "  mapped_arr = []\n",
        "  for a in arr:\n",
        "      if a in second_items:\n",
        "          index = second_items.index(a)\n",
        "          mapped_arr.append(first_items[index])\n",
        "      else:\n",
        "          mapped_arr.append(a)\n",
        "\n",
        "  return mapped_arr"
      ],
      "metadata": {
        "id": "mhpZFF5Va_Da"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sequence(args,clasifier):\n",
        "    line, candidate_labels = args\n",
        "    result = clasifier(line, candidate_labels)\n",
        "    highest_score_index = result['scores'].index(max(result['scores']))\n",
        "    highest_score_label = result['labels'][highest_score_index]\n",
        "    highest_score = max(result['scores'])\n",
        "\n",
        "    return result['labels'][0]\n",
        "    # return highest_score_label, highest_score\n",
        "    # print(line, highest_score_label, '\\n', highest_score, '\\n', result['scores'])\n",
        "    # print('<--------------------------------->')"
      ],
      "metadata": {
        "id": "0zmq4PlOEqZh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_classification(input_text, mapping, total, model, save, isFile):\n",
        "  classifier = pipeline(\"zero-shot-classification\", model)\n",
        "  candidate_labels = get_labels(mapping)\n",
        "\n",
        "  if isFile == 'y':\n",
        "    df = pd.read_csv(input_text)\n",
        "    # df = df.iloc[0:total] # first total lines\n",
        "    # df = df.sample(n=total, random_state=1) # random total lines\n",
        "    true_labels = df['updated_label'].tolist()\n",
        "    true_labels = map_mask_to_understandable(true_labels)\n",
        "    df = df['source_article']\n",
        "    # print('T = ', '\\n', true_labels)\n",
        "    # print('Tlen = ', len(true_labels))\n",
        "    # print('C = ', '\\n', candidate_labels)\n",
        "\n",
        "    cnt = 1\n",
        "    predictions = []\n",
        "\n",
        "    for line in df:\n",
        "      prediction = classify_sequence([line, candidate_labels],classifier)\n",
        "      predictions.append(prediction)\n",
        "      # print(cnt, '/', total)\n",
        "      print(f'\\r{round((cnt / total) * 100, 2)}% / 100% (', cnt , ')', end='', flush=True)\n",
        "      cnt += 1\n",
        "\n",
        "    predictions = map_mask_to_understandable(predictions)\n",
        "    # print('P = ', '\\n', predictions)\n",
        "    # print('Plen = ', len(predictions))\n",
        "    all_labels = map_mask_to_understandable(candidate_labels)\n",
        "\n",
        "    # unique_labels = list(set(true_labels + predictions))\n",
        "    report = classification_report(true_labels, predictions, labels = all_labels)\n",
        "    print('\\n', report)\n",
        "\n",
        "    matrix = confusion_matrix(true_labels, predictions, labels=all_labels)\n",
        "    sb.heatmap(matrix, xticklabels=all_labels, yticklabels=all_labels, annot=True, fmt=\"d\")\n",
        "    plt.xticks(rotation=90)\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "    if save == 'y':\n",
        "      plt.savefig('results/heatmap.png', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    result_df = pd.DataFrame({\n",
        "        'actual': true_labels,\n",
        "        'prediction': predictions\n",
        "    })\n",
        "    if save == 'y':\n",
        "      result_df.to_csv('results/act_pred.csv')\n",
        "      report_df = pd.DataFrame([report])\n",
        "      report_df.to_csv('results/report.csv', index=False)\n",
        "      !zip -r results.zip 'results'\n",
        "  else:\n",
        "      prediction = classify_sequence([input_text, candidate_labels],classifier)\n",
        "      print(prediction)"
      ],
      "metadata": {
        "id": "av-tacNL2mNe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEMO"
      ],
      "metadata": {
        "id": "9uLd3m8si94u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(\n",
        "    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner', 'coref'],\n",
        "    memory='4G',\n",
        "    endpoint='http://localhost:9001',\n",
        "    be_quiet=True)\n",
        "print(client)\n",
        "client.start()"
      ],
      "metadata": {
        "id": "Nrgzc7Qknsa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe632e3-70fc-4876-9d76-0343bf85d98e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Writing properties to tmp file: corenlp_server-10f3169ebde040ad.props\n",
            "INFO:stanza:Starting server with command: java -Xmx4G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-10f3169ebde040ad.props -annotators tokenize,ssplit,pos,lemma,ner,coref -preload -outputFormat serialized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<stanza.server.client.CoreNLPClient object at 0x7fd3b431b280>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the pre-trained model\n",
        "# with open(\"nb_classifier.pickle\", \"rb\") as dmp:\n",
        "#     nb = pickle.load(dmp)\n",
        "#     print(\"[I] NB classifier loaded from a file\")\n",
        "\n",
        "\n",
        "cos_sim_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "mapping = 'mapping.json'\n",
        "model = 'cross-encoder/nli-distilroberta-base'\n",
        "# model = 'facebook/bart-large-mnli'\n",
        "# model = 'facebook/bart-large-mnli'\n",
        "# model = \"google/electra-large-discriminator\"\n",
        "save = 'y'\n",
        "isFile = 'n'\n",
        "total = 1\n",
        "# If Joe eats greasy food, he will feel sick. Given now that Joe feels sick, therefore, Joe must have had greasy food\n",
        "# Jack is a good athlete. Jack comes from Canada. Therefore, all Canadians are good athletes\n",
        "while True:\n",
        "    text = input(\"\\nEnter a text to classify: \")\n",
        "    if len(text) == 0: break\n",
        "\n",
        "    # Extract text features for classification\n",
        "    masked_array =  mask_text([text], client, cos_sim_model)\n",
        "    print(\"\\nMasked text:\", masked_array[0], \"\\n\")\n",
        "    input_text = masked_array[0]\n",
        "\n",
        "\n",
        "    do_classification(input_text, mapping, total, model, save, isFile)\n"
      ],
      "metadata": {
        "id": "-mMPqbw_kfTk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "d65a3397-d779-4f4f-ed68-880d225e6cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter a text to classify: Annie must like Starbucks because all white girls like Starbucks.\n",
            "\n",
            "Masked text: Annie must [MSK2] [MSK1] because all white girls [MSK2] [MSK1]. \n",
            "\n",
            "[MSK1] is true because of [MSK2]. [MSK2] is true because of [MSK1].\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-6434cad3d6f6>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Jack is a good athlete. Jack comes from Canada. Therefore, all Canadians are good athletes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter a text to classify: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.stop()"
      ],
      "metadata": {
        "id": "b2inUb2Mnu7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "ZAKDY0Fd6lyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pieliku klat profileri (visu laiku patere coref_mask_and_tokenize() un get_similar_spans());\n",
        "- Uzliku skaistaku progresa printu;\n",
        "- Pieliku get_stopwords klat paris stopwordus;\n",
        "- Ieliku savu format_text, jo bija problemas ar lielajiem burtiem, punktiem, pēdiņām utt.\n",
        "- Pieliku metodi remove_quotes.\n",
        "- Pamainiju mask_text faila izveidi.\n",
        "- output.csv fails ar 1108 rindinam:\n",
        "https://github.com/HaraldsU/VTP_LPD/blob/main/output.csv\n"
      ],
      "metadata": {
        "id": "_GvdYI2zx0Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/HaraldsU/VTP_LPD/main/Data/input_full.csv\" -O 'input_full.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wB89O0V5HVE",
        "outputId": "f1f15826-252c-474a-e2b9-293a6db400dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-18 23:18:16--  https://raw.githubusercontent.com/HaraldsU/VTP_LPD/main/Data/input_full.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 865823 (846K) [text/plain]\n",
            "Saving to: ‘input_full.csv’\n",
            "\n",
            "input_full.csv      100%[===================>] 845.53K  4.25MB/s    in 0.2s    \n",
            "\n",
            "2024-06-18 23:18:16 (4.25 MB/s) - ‘input_full.csv’ saved [865823/865823]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_excessive_lines(df, max_words=100):\n",
        "    # Filter rows where the word count in 'source_article' is less than or equal to max_words\n",
        "    df_filtered = df[df['source_article'].apply(lambda x: len(x.split()) <= max_words)]\n",
        "    return df_filtered"
      ],
      "metadata": {
        "id": "dnDoghEmjssz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = [\"Jack is a good athlete. Jack comes from Canada. Therefore, all Canadians are good athletes. Good athletes always win.\"] # [MSK1] is a [MSK2]. [MSK1] comes from [MSK3]. Therefore, all [MSK3] are [MSK2].\n",
        "# text = ['If Joe eats greasy food, he will feel sick. Given now that Joe feels sick, therefore, Joe must have had greasy food.']\n",
        "# text = [\"McDonald's Hamburgers: over 99 billion served.\"]\n",
        "# text = ['men don’t cry']\n",
        "# text = ['David is so wrong about Luna\\'s work ethic. David is just an egotistical jerk with a God complex, what does he know?']\n",
        "# text  = ['Mob of people: Lower taxes! Lower taxes! Politician: People, your taxes are high because of illegal immigrants. That\\'s right—illegal immigrants. We need to get rid of them. Mob of people: (murmuring amongst themselves) Hmmm... immigrants. Let\\'s get rid of them!']\n",
        "# text  = ['Trump cites (biased) poll results showing that people think he\\'s a strong leader to prove a point that he is a strong leader. ABC News/Washington Post Poll (wrong big on election) said almost all stand by their vote on me & 53% said strong leader.']\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(\n",
        "    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner', 'coref'],\n",
        "    memory='4G',\n",
        "    endpoint='http://localhost:9001',\n",
        "    be_quiet=True)\n",
        "print(client)\n",
        "client.start()\n",
        "\n",
        "\n",
        "# df = pd.read_csv('input.csv')\n",
        "# df = df[0:100]\n",
        "# selected_rows = df.iloc[0:99] # First 99 rows\n",
        "\n",
        "df = pd.read_csv('input_full.csv')\n",
        "df_reduced = remove_excessive_lines(df,100)\n",
        "df_reduced.to_csv('input_reduced.csv', index=False)\n",
        "\n",
        "rows = df_reduced['source_article']\n",
        "\n",
        "# rows = text\n",
        "print('Row count = ', len(rows))\n",
        "\n",
        "# %lprun -f mask_text_file mask_text_file(rows, client, \"output_full.csv\", 'w', model)\n",
        "mask_text_file(rows, client, \"output_reduced.csv\", 'w', model)\n",
        "client.stop()"
      ],
      "metadata": {
        "id": "zaQTHCljxO_5",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbeaa8c-ce3b-45e7-df3a-c742ada4c105"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "INFO:stanza:Writing properties to tmp file: corenlp_server-a63622d9182b48c3.props\n",
            "INFO:stanza:Starting server with command: java -Xmx4G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-a63622d9182b48c3.props -annotators tokenize,ssplit,pos,lemma,ner,coref -preload -outputFormat serialized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<stanza.server.client.CoreNLPClient object at 0x7f9663c472e0>\n",
            "Row count =  2210\n",
            "15.57% / 100% ( 344 )\n",
            "No valid spans\n",
            "34.43% / 100% ( 761 )\n",
            "No valid spans\n",
            "39.46% / 100% ( 872 )\n",
            "No valid spans\n",
            "74.62% / 100% ( 1649 )\n",
            "No valid spans\n",
            "75.16% / 100% ( 1661 )\n",
            "No valid spans\n",
            "76.33% / 100% ( 1687 )\n",
            "No valid spans\n",
            "100.0% / 100% ( 2210 )\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = pd.read_csv('output_reduced.csv')\n",
        "input = pd.read_csv('input_reduced.csv')\n",
        "\n",
        "input['source_article'] = output\n",
        "input.to_csv('output_reduced.csv', index=False)\n"
      ],
      "metadata": {
        "id": "fMXvAZoRH7qD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForPreTraining, AutoModelForCausalLM, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "ClumHhR2Nu2E"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  mapping = 'mapping.json'\n",
        "  input_text = 'output_full.csv'\n",
        "  # input_text = 'Hah aha I am you are.'\n",
        "  # model = 'cross-encoder/nli-distilroberta-base'\n",
        "  # model = AutoModelForPreTraining.from_pretrained(\"google/electra-small-discriminator\")\n",
        "  # model = AutoModelForPreTraining.from_pretrained(\"google/electra-large-discriminator\")\n",
        "  # model = AutoModelForSequenceClassification.from_pretrained(\"sileod/deberta-v3-small-tasksource-nli\")\n",
        "  model = \"cross-encoder/nli-distilroberta-base\" # Even faster\n",
        "  save = 'y'\n",
        "  isFile = 'y'\n",
        "  total = 2222\n",
        "\n",
        "  do_classification(input_text, mapping, total, model, save, isFile)"
      ],
      "metadata": {
        "id": "PPjSCR4Kw7rO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}