{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaraldsU/VTP_LPD/blob/main/Val_teh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Rf0z_OGSrocH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJMg1XACw9qD",
        "outputId": "0f4883b6-9803-4d16-f6fa-d843e322091a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.8.2-py3-none-any.whl (990 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji (from stanza)\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, emoji, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stanza\n",
            "Successfully installed emoji-2.12.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 stanza-1.8.2\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install line_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oovNpO99txAs",
        "outputId": "2a768049-f6d0-42d6-afd9-6ed29f686cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting line_profiler\n",
            "  Downloading line_profiler-4.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (717 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.6/717.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: line_profiler\n",
            "Successfully installed line_profiler-4.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext line_profiler"
      ],
      "metadata": {
        "id": "SdNVNYOmt_Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\" -O 'stopwords.txt'\n",
        "!wget \"https://raw.githubusercontent.com/HaraldsU/VTP_LPD/main/input.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeBTV_d-G_ki",
        "outputId": "c0fb3ba5-49e8-4c18-9849-5531d29c3cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-17 13:38:24--  https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 622 [text/plain]\n",
            "Saving to: ‘stopwords.txt’\n",
            "\n",
            "stopwords.txt       100%[===================>]     622  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-17 13:38:25 (36.0 MB/s) - ‘stopwords.txt’ saved [622/622]\n",
            "\n",
            "--2024-06-17 13:38:25--  https://raw.githubusercontent.com/HaraldsU/VTP_LPD/main/input.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 417391 (408K) [text/plain]\n",
            "Saving to: ‘input.csv’\n",
            "\n",
            "input.csv           100%[===================>] 407.61K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-06-17 13:38:25 (9.00 MB/s) - ‘input.csv’ saved [417391/417391]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import re\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Download the Stanford CoreNLP package with Stanza's installation command\n",
        "# This'll take several minutes, depending on the network speed\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "from stanza.server import CoreNLPClient\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir"
      ],
      "metadata": {
        "id": "QJgF8nuiJZlq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "zBGKntzUr0_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_text(output):\n",
        "  # print_help(output, 'Op')\n",
        "  regexp = re.compile(r'[MSK\\d*]')\n",
        "  regexp2 = re.compile(r\"'[A-Za-z]\")\n",
        "  final_output = []\n",
        "\n",
        "  for word in range(len(output)):\n",
        "    prev = output[word - 1]\n",
        "    pprev = output[word - 2]\n",
        "    cur = output[word]\n",
        "\n",
        "    if not regexp.search(cur):\n",
        "      if word == 0:\n",
        "        cur = cur.capitalize()\n",
        "      elif word > 1 and prev in ['.', '!', '?']:\n",
        "        cur = cur.capitalize()\n",
        "\n",
        "    if (cur in [\".\", \",\", '!', '?', ':', ';'] and final_output) or regexp2.search(cur):\n",
        "      final_output[-1] += cur\n",
        "    elif cur == '-':\n",
        "      final_output[-1] += cur\n",
        "      final_output[-1] += output[word + 1]\n",
        "      word += 1\n",
        "    elif word == len(output) - 1 and cur not in ['.', '?', '!', '\\'']:\n",
        "      final_output[-1] += '.'\n",
        "    else:\n",
        "        final_output.append(cur)\n",
        "\n",
        "  return ' '.join(final_output)"
      ],
      "metadata": {
        "id": "oDA1lV3ywA6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spans_to_ranges(spans):\n",
        "  msk_gr = []\n",
        "  for span in spans:\n",
        "    span1_start = span['span1']['start']\n",
        "    span1_end = span['span1']['end']\n",
        "    span2_start = span['span2']['start']\n",
        "    span2_end = span['span2']['end']\n",
        "    in_grp = False\n",
        "    c = 0\n",
        "    for gr in msk_gr:\n",
        "      for rng in gr:\n",
        "        if span1_start<=rng[1] and span1_end >= rng[0]:\n",
        "          c+=1\n",
        "        if span2_start<=rng[1] and span2_end >= rng[0]:\n",
        "          c+=2\n",
        "        if  c == 3 : break\n",
        "      if    c == 0 :\n",
        "        continue\n",
        "      elif  c == 1 :\n",
        "        gr.append([span2_start,span2_end])\n",
        "        break\n",
        "      elif  c == 2 :\n",
        "        gr.append([span1_start,span1_end])\n",
        "        break\n",
        "      elif  c == 3 :\n",
        "        break\n",
        "    if c == 0: msk_gr.append([[span1_start,span1_end],[span2_start,span2_end]])\n",
        "  return msk_gr"
      ],
      "metadata": {
        "id": "l-3TIASat5hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_spans(spans, cos_threshold, model):\n",
        "  embedings =  model.encode([s[\"text\"] for s in spans])\n",
        "  similar_spans = []\n",
        "  for i in range(len(spans)-1):\n",
        "    emb = embedings[i]\n",
        "    for j in range(i+1,len(spans)):\n",
        "      if spans[j]['start'] <= spans[i]['end']:\n",
        "        continue\n",
        "      emb_chk = embedings[j]\n",
        "      cos_score = cosine_similarity([emb], [emb_chk])\n",
        "      if(cos_score>=cos_threshold):\n",
        "        score = 1*cos_score + 0.15*(1-1/(spans[i]['end']+1-spans[i]['start']+spans[j]['end']+1-spans[j]['start'])) # cos_score +  length of text\n",
        "\n",
        "        similar_spans.append({\n",
        "            'score': score, 'cos:':cos_score\n",
        "            ,'span1':spans[i]\n",
        "            ,'span2':spans[j]})\n",
        "  similar_spans = sorted(similar_spans, key=lambda d: d['score'], reverse=True)\n",
        "  return similar_spans"
      ],
      "metadata": {
        "id": "5nXmZXiitn9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spans(words, stopwords):\n",
        "  spans=[]\n",
        "  regexp = re.compile(r'[MSK\\d*]')\n",
        "  range(len(words))\n",
        "  for i in range(len(words)):\n",
        "    for j in range(i+1, len(words)+1):\n",
        "      c = False\n",
        "      for w in words[i:j]:\n",
        "        if w in stopwords:\n",
        "          c = True\n",
        "          break\n",
        "      if c: continue\n",
        "\n",
        "      s = ' '.join(words[i:j])\n",
        "      # s=s.replace(' ,','')\n",
        "      if(regexp.search(s) or s ==','):\n",
        "        continue\n",
        "      spans.append({\"text\":s, \"start\":i, \"end\": j-1})\n",
        "  return spans"
      ],
      "metadata": {
        "id": "wKSBSedKsYWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coref_mask_and_tokenize(text, core_nlp_client):\n",
        "  msk_id=0\n",
        "  document = core_nlp_client.annotate(text)\n",
        "  corefs = document.corefChain\n",
        "  resolved = []\n",
        "  chain_mask = {}\n",
        "  for chain in corefs:\n",
        "    if len(chain.mention) == 1:\n",
        "      continue\n",
        "    msk_id+=1\n",
        "    chain_mask[chain.chainID]='[MSK'+str(msk_id)+']'\n",
        "\n",
        "  words = []\n",
        "  words_lemmatized = []\n",
        "  for sentence in document.sentence:\n",
        "    for token in sentence.token:\n",
        "        corefClustId = token.corefClusterID\n",
        "        chain = None\n",
        "        for c in corefs:\n",
        "          if c.chainID == corefClustId:\n",
        "            chain = c\n",
        "            break\n",
        "\n",
        "        if chain is None or len(chain.mention) == 1:\n",
        "          word = token.word.lower()\n",
        "          word_l = token.lemma\n",
        "        else:\n",
        "          word = chain_mask.get(chain.chainID)\n",
        "          word_l = word\n",
        "        words.append(word)\n",
        "        words_lemmatized.append(word_l)\n",
        "  return words, words_lemmatized, msk_id\n"
      ],
      "metadata": {
        "id": "OjcDbdb-up03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_stopwords():\n",
        "#   text_file = open(\"stopwords.txt\", \"r\")\n",
        "#   stopwords = text_file.read().splitlines()\n",
        "#   stopwords.extend(['.',',','!','?','therefore','if', 'given', 'give', ':', '\\'', '\\\"', ' '])\n",
        "#   text_file.close()\n",
        "#   return stopwords\n",
        "\n",
        "def get_stopwords():\n",
        "    # Open the file and read the stopwords\n",
        "    with open(\"stopwords.txt\", \"r\") as text_file:\n",
        "        stopwords = text_file.read().splitlines()\n",
        "\n",
        "    # List of additional stopwords to add\n",
        "    additional_stopwords = ['.', ',', '!', '?', 'therefore', 'if', 'given', 'give',\n",
        "                            ':', '\\'', '\\\"', '”']\n",
        "\n",
        "    # Extend the stopwords list if they are not already present\n",
        "    for word in additional_stopwords:\n",
        "        if word not in stopwords:\n",
        "            stopwords.append(word)\n",
        "\n",
        "    # Write the updated list back to the file\n",
        "    with open(\"stopwords.txt\", \"w\") as text_file:\n",
        "        for word in stopwords:\n",
        "            text_file.write(word + \"\\n\")\n",
        "\n",
        "    return stopwords"
      ],
      "metadata": {
        "id": "VYO2R4KZyZ7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_quotes(text):\n",
        "    result = []\n",
        "    for word in text:\n",
        "        new_word = word.replace('\"', '').replace('”', '').replace('“', '')\n",
        "        result.append(new_word)\n",
        "    return result"
      ],
      "metadata": {
        "id": "w9QVJs3_bH8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_help(obj, name='Obj'):\n",
        "  print(name, '=', end=\" \")\n",
        "  for item in obj:\n",
        "    print(item, end =\" \"),\n",
        "  print()"
      ],
      "metadata": {
        "id": "bW1cawz0YA3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_word_ranges(words, mask_ranges, msk_index):\n",
        "  d = dict(enumerate(map(str, words)))\n",
        "  for msk in mask_ranges:\n",
        "    msk_index+=1\n",
        "    msk_txt = '[MSK'+str(msk_index)+']'\n",
        "    for r in msk:\n",
        "      d[r[0]]= msk_txt\n",
        "      for i in range(r[0]+1,r[1]+1):\n",
        "        d.pop(i)\n",
        "  arr = [v for k, v in d.items()]\n",
        "  return arr"
      ],
      "metadata": {
        "id": "1qgib-BoT21X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_text(text_arr, core_nlp_client, out_file_name, write_mode, model):\n",
        "  stopwords = get_stopwords()\n",
        "\n",
        "  text_arr = remove_quotes(text_arr)\n",
        "\n",
        "  result = []\n",
        "  write_queue=[]\n",
        "  total = len(text_arr)\n",
        "  i = 0\n",
        "\n",
        "  for text in text_arr:\n",
        "    i+=1\n",
        "    words, words_lemmatized, mask_count = coref_mask_and_tokenize(text, core_nlp_client)\n",
        "    spans = get_spans(words_lemmatized, stopwords)\n",
        "    print(f'\\r{round((i / total) * 100, 2)}% / 100% (', i , ')', end='', flush=True)\n",
        "\n",
        "    if not spans:\n",
        "      result=text\n",
        "      write_queue.append(result+'\\n')\n",
        "      print('\\nNo valid spans')\n",
        "      continue\n",
        "\n",
        "    similar_spans = get_similar_spans(spans, .91, model)\n",
        "    mask_ranges = spans_to_ranges(similar_spans)\n",
        "    maksed_words = mask_word_ranges(words, mask_ranges, mask_count)\n",
        "    result = format_text(maksed_words)\n",
        "    write_queue.append(result + '\\n')\n",
        "\n",
        "  write_queue_df = pd.DataFrame(write_queue)\n",
        "  write_queue_df.to_csv(out_file_name, index=False, quoting=1)\n",
        "  print('\\nFinished!')\n"
      ],
      "metadata": {
        "id": "ulfB7oCSr_XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Server"
      ],
      "metadata": {
        "id": "E9HDNcCIHoIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(\n",
        "    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner', 'coref'],\n",
        "    memory='4G',\n",
        "    endpoint='http://localhost:9001',\n",
        "    be_quiet=True)\n",
        "print(client)\n",
        "client.start()"
      ],
      "metadata": {
        "id": "3Vyq2kQBxKAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute"
      ],
      "metadata": {
        "id": "ZAKDY0Fd6lyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pieliku klat profileri (visu laiku patere coref_mask_and_tokenize() un get_similar_spans());\n",
        "- Uzliku skaistaku progresa printu;\n",
        "- Pieliku get_stopwords klat paris stopwordus;\n",
        "- Ieliku savu format_text, jo bija problemas ar lielajiem burtiem, punktiem, pēdiņām utt.\n",
        "- Pieliku metodi remove_quotes.\n",
        "- Pamainiju mask_text faila izveidi.\n",
        "- output.csv fails ar 1108 rindinam:\n",
        "https://github.com/HaraldsU/VTP_LPD/blob/main/output.csv\n"
      ],
      "metadata": {
        "id": "_GvdYI2zx0Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Jack is a good athlete. Jack comes from Canada. Therefore, all Canadians are good athletes. Good athletes always win.\" # [MSK1] is a [MSK2]. [MSK1] comes from [MSK3]. Therefore, all [MSK3] are [MSK2].\n",
        "text = \"If Joe eats greasy food, he will feel sick. Given now that Joe feels sick, therefore, Joe must have had greasy food.\" #\n",
        "# text = [\"McDonald's Hamburgers: over 99 billion served.\"]\n",
        "# text = ['men don’t cry']\n",
        "# text = ['David is so wrong about Luna\\'s work ethic. David is just an egotistical jerk with a God complex, what does he know?']\n",
        "\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "df = pd.read_csv('input.csv')\n",
        "# selected_rows = df.iloc[0:99] # First 99 rows\n",
        "rows = [text]#df['source_article']\n",
        "print('Row count = ', len(rows))\n",
        "\n",
        "%lprun -f mask_text mask_text(rows, client, \"output.csv\", 'w', model)"
      ],
      "metadata": {
        "id": "zaQTHCljxO_5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.stop()"
      ],
      "metadata": {
        "id": "FbgZVAWexUKs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}